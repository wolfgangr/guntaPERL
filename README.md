# Guntamatic data logging


## Example

![screenshot](/images/my-rrd-log-2021-01-15.png)


## Purpose

This repo documents my endeavour to
* retrieve the status info from a Guntamatic(TM?) Powerchip 50 (TM?) wood chip cauldron 
* store the status in a round robin rrd data base - RTFM: https://oss.oetiker.ch/rrdtool/
* provide a web interface to retrieve and browse the data

Goal is to understand the behaviour of the system over time for optimization and to identify undesired states.

The manufacturerer offers a cloud based web service and a mobile client.  
I was not satisfied with the plotting capabilities of both, because of  
* **limited data subset**
* **only one item plotted per chart**, so it's difficult to undestand functional interrelations 
* long, odd, changing plot interval (sth. like e. g. 53 min)
* **all history is lost** after some days and after a restart of the controller

However, the manufacturer's tools allow some remote alarm and control of the cauldron, which I like and do not intend to implement.  
I can't see a reason why not to use those tools in parallel with my charting framework.  

There are different logging and plotting solutions out there, e.g. in the realm of home management frameworks (*FHEM*, just to name one) or generic rrd data collectors (*cacti* beeing the best known one).  
May be, they may provide a flatter early learning curve towards some simple subset of what I have done.  
To my impression, at a certain level of customization, they provide more obstacles by adding skd. of "obscurity" layer.  
In the end, any unconstrained turing capable generic programming language inevitably provides more flexibility.  

Last but not least, I was looking for a nice training environment to understand elaborated `rrd` configuration.  

## Setup

The cauldron controller has a web interface with DHCP configured by default.  
There is a MODBUS API as well, but I was perfectly happy with the HTTP API.  

My cauldron's controller runs Sotware v `32f`.   
It requires a key for API access, which has to be generated by Guntamatic support.  
They also may send you a API manual.  

The key seems to be derived from the serial number, so there is no point in reusing it.  
Earlier versions were reported to provide data access witout a special key or with a generic one, but I was not able to reproduce such instructions.  
  
There are five addresses I worked with, belonging to two slightly different API:  
* `http://w.x.y.z/daqdesc.cgi?key=....` \n separated mapping information   
* `http://w.x.y.z/daqdata.cgi?key=....` \n separated live data
* `http://w.x.y.z/ext/daqdesc.cgi?key=....` JSON mapping 
* `http://w.x.y.z/ext/daqdata.cgi?key=....` JSON live data   
* `http://w.x.y.z/par.cgi` skd. of exhaustive static config info, which seems loosly be related to the protocol...
  
I started with the JSON API, because this is the ine documented, and the format allowed an one-line-`wget`-command in `crontab` to collect log files.  
It was until I started parsing, rrd logging and pretty printing when I found out that the **JSON API** delivered only a **subset** of the data available at the **newline API**.  
While all parameters that refer to building installation are there (upper half of screenshot above), essential internal cauldron and chip feed information (bottom of the picture) is missing.  
  
So I switched to the newline-API halfway during the project.  
Files tagged with `plain...` refer to the newline-API.   
Half baken bretheren of those witout the plain-tagging may still be lingering around.
I kept that JSON versions for reference and maybe later code recycling.  

## Key elements of the code
### `config_plain.pm` 
is where core information matching the protocol is kept.  
It keeps **structure in sync** between the API, rrd, the logger and the renderer.   
Interface configurations at other 'edges' of the installation, however, may be kept in a decentralize manner in the header area of any of the scripts.

The config is derived from the JSON daqdesc, but completed and added up with further processing decisions.
The goal was to change fields, rrds, plotting style etc without fiddling in the hot code.
My gutt feels that I made that for some 80 % or so. I'd not expect seamless work in a fresh setup.

The config is ~~included~~ require'd the PERLie way into most other parts.
So I must be sure it meets **PERL syntax** on any chages.  
I used `parse-confi-plain.pl` to check for syntax errors and to debug available config data structures.  
Key and host name are kept in a separate `secret.pm`, with a github'bed anonymized copy of it.
  
### `library.pm` 
- well, a library, as it's name says.  ...
  
### Generator scripts

There are some `config-foo-bar.sh|pl` perl or shell scripts which **`setup_.*.rrd`** databases, rendering **`*.rrd-graph` templates** and the like.  
I used them as a reproducible way for setup and controlled modification.  
They are supposed to run only at setup time, or repeatedly during development cycles.  
Once stable, I prefer to set them as **not executable** as a protection against accidential deletion of rrd.  

### polling and logging demon **`log2rrd.pl`**

The main worker. It polls the cauldron controller's http server once per minute and logs into several rrd databases to be created under `rrd/`.  
After some trials I ended up with **60 s polling rate**.  
It proove to be a good idea to have the **step**, the DS **hearbeat** and the first level of **RRA consolidation** CF **in sync** with that rate. **Log times** are rounded to integer multiples of **minutes**.  

Why? Half of the variables are **boolean** states or integer encoded **enums**.  
Without proper alignment, I end up with **fractional values** in the rrd, even if choosing LAST as CF C_onsolidation F_unction.  
For reasons: RTFM rrd. e.g. this one http://rrdtool.vandenbogaerdt.nl/process.php at least 3 times...  
And believe the boss:  
http://rrd-mailinglists.937164.n2.nabble.com/How-To-Do-A-State-Change-Log-Graph-td1078946.html#a1079016  

In addition to rrd, any change in boolean or enum variable is logged to a file as status change.  
Later in the plots, those status change events appear as vertical markers through all the charts.

When I started with a 5 min polling interval, I had called it by cron each time.  
I had to learn that I miss loads of information regarding the ignition process, chip feeding or pump control.
At a 60 s interval, I could see considerable CPU load due to PERL startup overhead.  
I could cut this down by running the poller as a background demom.  
Now there is a `watchdog.sh` `cron` at some interval to check and - if necessary - revivify the poller.  


### Widget and Gadgets

There are two tools I keep reusing during different rrd machine surveillance projects.   
I decided to throw them into sth. like `/usr/local/bin/`.

#### `rrdtest.pl` 
is skd of extended frontend to `rrdtool lastupdate` and queries the last update of several rrd in a human readable way.  
```
~/guntamatic$ ./rrdtest.pl 240 rrd/*.rrd
===    gracetime: 240    =    now: 2021-01-16 23:49:32    =    diff: 2021-01-16 23:45:32    ===
--- [ rrd/status.rrd ] ---------------------------------  
        OK  (32s)       |  prog_main prog_HK1 prog_HK2 enbl opmode S_op SP_buf0 SP_hw0 S_P1 S_P2 op_hr
2021-01-16 23:49:00     |       1 4 4 1 0 0 0 0 1 1 1649 
--- [ rrd/statusX.rrd ] ---------------------------------  
        OK  (32s)       |  fault0 fault1 frflp level stb tks1 ign_vnt ign_ht
2021-01-16 23:49:00     |       U U 1 1 1 1 0 0 
--- [ rrd/temps.rrd ] ---------------------------------  
        OK  (32s)       |  pc_buf pc_pwr CO2 T_cald T_hw0 T_buf_top T_buf_bot T_out T_P1 T_P2
2021-01-16 23:49:00     |       27 0 17.98 67.63 63.03 59.61 33.95 -1.24 46.29 39.47 
--- [ rrd/tempsX.rrd ] ---------------------------------  
        OK  (32s)       |  T_ret pc_exh pc_vent pc_stok I_stok pc_aug1 I_aug1 pc_grt
2021-01-16 23:49:00     |       55.96 0 0 0 0 0 0 0 
 =============== DONE - errors: 0 ==============
 ```
It has a configurable "gracetime" and reports the state upon return to the caller.  
This is essential for my watchdog, to figure out whether updates are overdue.  
  
I also like running it as `watch -n1 ./rrdtest.pl *.rrd` on a separate console window when debugging rrd data capture, like the poller. 
Thus I always have a look what's ging on, when and what was updated into my rrds and can even cut'n paste field names into edited scripts.

#### `rrd2csv.pl`
a wrapper to `rrdtool fetch`

```
$ rrd2csv.pl -h
/usr/local/bin/rrd2csv.pl:

retrieve data from RRD and output them as CSV to file or STDOUT

usage: /usr/local/bin/rrd2csv.pl db.rrd CF
  [-s start][-e end][-r res][-a]  [-V valid-rows ]
  [-f outfile][-x sep][-d delim][-t][-T dttag][-z tz] [-H][-M]   
  [-v #][-h]
 

        for further details, see RRDtool fetch for details

        db.rrd
                rrd file name to retrieve data from

        CF      rrd CF (AVERAGE,MIN,MAX,LAST)

        -s starttime
                transparently forwarded to RRDtool, 
                default NOW - 1 day

        -e endtime
                transparently forwarded to RRDtool,
                default NOW

        -r res 
                resolution (seconds per value)
                default is highest available in rrd

        -a align
                adjust starttime to resolution

        -V valid rows
                preselect rows by NaN'niness
                (integer) minimum valid fields i.e not NaN per row
                0 - include all empty (NaN only) rows
                1 - (default ) at least one not-NaN - don't loose any information
                up to num-cols - fine tune information vs data throughput
                negative integers: complement count top down e.g.
                -1 - zero NaN allowed
                -2 - one NaN allowed

                        [-f outfile] [-h] [-H] [-x sep] [-d delim]

        -f output file
                default ist STDOUT if omitted

        -x ;    CSV field separator, default is  ';'

        -d "    CSV field delimiter, default is ''

        -t      include header tag line

        -T foo  header line time tag, default ist 'time'

        -H      translate unixtime to H_uman readable time
        -M      translate unixtime to M_ySQL timestamps
        -z foo  set timezone, default is 'local'

        -v int  set verbosity level

        -h      print this message
```

This allows configurable extraction of rrd data in csv like human and/or machine readable format.  
I use it in other projects for rrd-to-SQL-upload.  
It neatly integrates with `mysqlimport`, so cron controlled database sync just requires a couple of bash lines.  


### Web Visualisation 

is provided by the `guntamatic.pl` script in  `/render`.  
It allows easy and fast moving and zooming through time scale. See the image on top of the page.  

There are buttons for half / full image with jumps back and forth, + / - zomm buttons, and a reset to standard view.
The time span can also be entered in input fields, which became my preferred way after I got familiar with the rrd time spec format.
Image size and time resoultion (considering rrd config) can be changed.


It must be configured as `CGI` with perl enabled in the web server. I use `lighttpd`.  
  
Temporary rrd graphs are stored in `/render/tmp`.  
This must be readable and writable by the web server.  
Group acces for `www-data` group works fine on my out-of-the-box-debian.  
Newer versions of charts overwrite older ones. This saves the need of temp cleanup.  
This is not really thread save, but that does not hurt in a low frequency machine surveillance environment.  
  
The render script relies on `rrd-graph` templates, which are nothing else but `rrdtool graph` commands with some leading lines omitted:   
When browsing through time, the renderer adds rrdtool command w/ options, rrd filename, start time, end time and a default chart size, as well as some vertical lines as event markers.  

The `test-...` templates are edited manually, derived from `drraw` created boiler plates.  

The `gen-...` template are generated by scripts in the main dir. I used this approach for the somewhat more complicated task to visualize the boolean and the enum fields.  
To tune them, I use four screens 
* one with vi on the creator script, 
* a console to rerun the creator after every debug modification, 
* a web browswer window to review the rendering result 
* a console to grep httpd error messages out of /var/log/messages in case of `internal server error`.  
CGI, you know... That's where I appreciate my dual screen installation...

The renderer also produces frames with a human readable list of all current state of variables and a subset of state change event log, matching the time frame displayed in the charts. They are created by silly little `log-range.pl` and `current-state.pl`

While I tried to catch some worst security holes on the fly, there was no thorough safety planning.  
I expect some open backdoors and hope I can rely on my LAN security...   

### System requirements

I have it running togehter with a couple of similiar projects on a 10 year old 'thin client' laptop style hardware single core 1200 MHz AMD G-T44R Processor and 4GB RAM. Mass storage is a 64 GB SSD. rrd as configured need 136 MB - so this is luxury.  
Nevertheless, it's wise to use `df` and `du` commands and maybe a calculator while playing with rrd configurations.  

There is a 64-bit debian 10.7 on the box.  
Completely headless except for bios configuration. 'Thin server', so to say.   
All PERL modules used are from debian main repo, no backports, no CPAN, no custom build.  

CPU load is << 5 % during polling and goes close to 100 % for a second or so when producing  charts.  
So I guess a raspberry could do the job as well, but I haven't tried.  
Personally, I don't like the idea of runnig rrd on SD-cards. Don't like mass storage, network and peripheral access all sharing the same USB Bus. File systems for 24/7 operations mounted over fragile USB plugs. But all that may be a matter of taste.


## Disclaimer:

This is all alpha-state *'works-for-me'*-level-stuff.  
**NEVER USE THIS IN A REAL WORD ENVIRONMENT!**  
... and if you do anyway, don't sue me.
**Expect stuff going wrong.**  
  
Don't expect seamless installation out of the box.  
Do not even try it without sound knowledge of linux, perl, rrd, shell script, CGI and networking.  
  
The Guntamatic-**API allows writes to the controller** as well.  
I did not explicitly try it, but can we be sure that this does not happen by accident?  
I's an **oven**, so, in the end, it's supposed to have **fire in it**.  
And you don't want to have the **fire outside**, do you?  
There is stuff that may break, and **moving parts (conveyor augers, e.g.!) that may seriously hurt people**.  
Or at least stop working, destroy warranty claims, keep you freezing all day and whatever other evil things.



